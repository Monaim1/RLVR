{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31029eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install docling trl peft accelerate bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62326ff",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad6c25",
   "metadata": {},
   "source": [
    "the dataset was generated by converting raw json patents into a PDF.\n",
    "\n",
    "The goal is to train an SLM to re-generate the gold json given it's pre-processed text format with Docling.\n",
    "\n",
    "The train and validation manifests are already pre-processed with DOCLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c828c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e91ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': \"Extract the following fields as JSON only (no extra text). Fields: {publication_number, application_number, patent_number, date_published, filing_date, patent_issue_date, abandon_date, decision, main_cpc_label, main_ipcr_label, title, abstract, summary, claims}\\n\\nDOCUMENT:\\n## Intelligent Drug and/or Fluid Delivery System to Optimizing Medical Treatment or Therapy Using Pharmacodynamic and/or Pharamacokinetic Data\\n\\nPatent Number:\\n\\n9950112\\n\\nApplication Number:\\n\\n13817165\\n\\nPublication Date:\\n\\nN/A\\n\\nApplicant:\\n\\nN/A\\n\\nInventors:\\n\\nN/A\\n\\nThis document contains information about the patent's abstract, claims, and detailed description.\\n\\n## Abstract\\n\\nA pharmacodynamic (PD), pharmacokinetic (PK), or both and PK guided infusion device, system and method optimizes the safety and efficacy of various forms of treatment or therapy (e.g., drug and/or fluid) in a variety of health-care and other settings.\\n\\n## Claims\\n\\nReturn strictly a single JSON object with those keys.\",\n",
       " 'gold': {'application_number': '13817165',\n",
       "  'publication_number': 'US20130296823A1-20131107',\n",
       "  'title': 'Intelligent Drug and/or Fluid Delivery System to Optimizing Medical Treatment or Therapy Using Pharmacodynamic and/or Pharamacokinetic Data',\n",
       "  'decision': 'ACCEPTED',\n",
       "  'date_published': '20131107',\n",
       "  'main_cpc_label': 'A61M51723',\n",
       "  'main_ipcr_label': 'A61M5172',\n",
       "  'patent_number': '9950112',\n",
       "  'filing_date': '20180219',\n",
       "  'patent_issue_date': '20180424',\n",
       "  'abandon_date': '',\n",
       "  'abstract': 'A pharmacodynamic (PD), pharmacokinetic (PK), or both and PK guided infusion device, system and method optimizes the safety and efficacy of various forms of treatment or therapy (e.g., drug and/or fluid) in a variety of health-care and other settings.'},\n",
       " 'patent_id': 'patent_US20130296823A1-20131107'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fields expected in gold JSONs\n",
    "RELEVANT_FIELDS: List[str] = [\n",
    "    \"publication_number\",\n",
    "    \"application_number\",\n",
    "    \"patent_number\",\n",
    "    \"date_published\",\n",
    "    \"filing_date\",\n",
    "    \"patent_issue_date\",\n",
    "    \"abandon_date\",\n",
    "    \"decision\",\n",
    "    \"main_cpc_label\",\n",
    "    \"main_ipcr_label\",\n",
    "    \"title\",\n",
    "    \"abstract\",\n",
    "    \"summary\",\n",
    "    \"claims\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_manifest(path: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "class PatentIEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for RLVR/GRPO IE on patent PDFs.\n",
    "\n",
    "    Expects a manifest DataFrame with columns:\n",
    "      - patent_id\n",
    "      - pdf_path\n",
    "      - gold_json_path\n",
    "      - text (optional if `preload_text=True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, manifest_df: pd.DataFrame, preload_text: bool = False):\n",
    "        self.df = manifest_df.reset_index(drop=True)\n",
    "        self.preload_text = preload_text\n",
    "\n",
    "        if self.preload_text and \"text\" not in self.df.columns:\n",
    "            # Pre-extract on the fly (prefer preprocessing pass for speed)\n",
    "            self.df = self.df.copy()\n",
    "            self.df[\"text\"] = self.df[\"pdf_path\"].apply(self._load_pdf_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _docling_converter(self):\n",
    "        try:\n",
    "            from docling.document_converter import DocumentConverter, InputFormat, PdfFormatOption\n",
    "            from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        return DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(\n",
    "                    pipeline_options=PdfPipelineOptions(\n",
    "                        do_ocr=False,\n",
    "                        force_backend_text=True,\n",
    "                        do_table_structure=False,\n",
    "                        generate_picture_images=False,\n",
    "                        generate_page_images=False,\n",
    "                        generate_table_images=False,\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _load_pdf_text(self, pdf_path: str) -> str:\n",
    "        converter = self._docling_converter()\n",
    "        if converter is not None:\n",
    "            res = converter.convert(str(pdf_path))\n",
    "            return res.document.export_to_text()\n",
    "        # Fallback to PyMuPDF if Docling unavailable\n",
    "        import fitz  # type: ignore\n",
    "        doc = fitz.open(pdf_path)\n",
    "        return \"\\n\\n\".join(page.get_text(\"text\") for page in doc)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        row = self.df.iloc[i]\n",
    "        text = (\n",
    "            row[\"text\"] if (\"text\" in row and self.preload_text) else self._load_pdf_text(row[\"pdf_path\"])\n",
    "        )\n",
    "        gold = json.load(open(row[\"gold_json_path\"], \"r\"))\n",
    "\n",
    "        fields_str = \", \".join(RELEVANT_FIELDS)\n",
    "        prompt = (\n",
    "            \"Extract the following fields as JSON only (no extra text). \"\n",
    "            f\"Fields: {{{fields_str}}}\\n\\n\"\n",
    "            f\"DOCUMENT:\\n{text}\\n\\n\"\n",
    "            \"Return strictly a single JSON object with those keys.\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_text\": prompt,\n",
    "            \"gold\": gold,\n",
    "            \"patent_id\": row[\"patent_id\"],\n",
    "        }\n",
    "\n",
    "\n",
    "train_manifest = load_manifest(\"Patent_Data/train_manifest.parquet\")\n",
    "val_manifest = load_manifest(\"Patent_Data/val_manifest.parquet\")\n",
    "\n",
    "patent_train_dataset = PatentIEDataset(train_manifest, preload_text=True)\n",
    "patent_val_dataset = PatentIEDataset(val_manifest, preload_text=True)\n",
    "patent_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed202e7",
   "metadata": {},
   "source": [
    "## Reward design for Information Extraction (IE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020222bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "\n",
    "\n",
    "RELEVANT_FIELDS: List[str] = [\n",
    "    \"publication_number\",\n",
    "    \"application_number\",\n",
    "    \"patent_number\",\n",
    "    \"date_published\",\n",
    "    \"filing_date\",\n",
    "    \"patent_issue_date\",\n",
    "    \"abandon_date\",\n",
    "    \"decision\",\n",
    "    \"main_cpc_label\",\n",
    "    \"main_ipcr_label\",\n",
    "    \"title\",\n",
    "    \"abstract\",\n",
    "    \"summary\",\n",
    "    \"claims\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _first_json(text: str) -> Optional[Dict[str, Any]]:\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "    start = text.find(\"{\")\n",
    "    if start == -1:\n",
    "        return None\n",
    "    depth, in_str, esc = 0, False, False\n",
    "    for i in range(start, len(text)):\n",
    "        ch = text[i]\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == \"\\\\\":\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "            elif ch == \"{\":\n",
    "                depth += 1\n",
    "            elif ch == \"}\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    try:\n",
    "                        return json.loads(text[start : i + 1])\n",
    "                    except Exception:\n",
    "                        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _norm(s: Any, max_len: int = 4000) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    if isinstance(s, (list, tuple)):\n",
    "        s = \"\\n\".join(map(str, s))\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len]\n",
    "    return s\n",
    "\n",
    "\n",
    "def _sim(a: str, b: str) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def _parse_date(s: Any) -> Optional[datetime]:\n",
    "    if not isinstance(s, str) or not s:\n",
    "        return None\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y%m%d\", \"%Y/%m/%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s.strip(), fmt)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_reward(\n",
    "    model_output_text: str,\n",
    "    gold: Dict[str, Any],\n",
    "    weights: Tuple[float, float, float, float] = (0.5, 0.4, 0.1, 0.05),\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    pred = _first_json(model_output_text)\n",
    "    validity = int(isinstance(pred, dict) and set(RELEVANT_FIELDS).issubset(set(pred.keys())))\n",
    "\n",
    "    # Field-level similarity (only where gold is non-empty)\n",
    "    sims: Dict[str, float] = {}\n",
    "    use_fields: List[str] = []\n",
    "    if isinstance(pred, dict):\n",
    "        for k in RELEVANT_FIELDS:\n",
    "            g = gold.get(k)\n",
    "            if g is None or (isinstance(g, str) and g.strip() == \"\"):\n",
    "                sims[k] = 0.0\n",
    "                continue\n",
    "            p = pred.get(k)\n",
    "            score = _sim(_norm(g), _norm(p))\n",
    "            sims[k] = float(score)\n",
    "            use_fields.append(k)\n",
    "    field_mean = sum(sims.get(k, 0.0) for k in use_fields) / max(1, len(use_fields))\n",
    "\n",
    "    # Constraints: dates in order if present\n",
    "    constraints = 0\n",
    "    if isinstance(pred, dict):\n",
    "        fd = _parse_date(pred.get(\"filing_date\"))\n",
    "        pd = _parse_date(pred.get(\"date_published\"))\n",
    "        id_ = _parse_date(pred.get(\"patent_issue_date\"))\n",
    "        ok = True\n",
    "        if fd and pd:\n",
    "            ok = ok and (fd <= pd)\n",
    "        if fd and id_:\n",
    "            ok = ok and (fd <= id_)\n",
    "        if pd and id_:\n",
    "            ok = ok and (pd <= id_)\n",
    "        constraints = int(ok)\n",
    "\n",
    "    # Format bonus: exact keys + ISO dates if present\n",
    "    fmt = 0.0\n",
    "    if isinstance(pred, dict) and set(pred.keys()) == set(RELEVANT_FIELDS):\n",
    "        iso_ok = True\n",
    "        for k in (\"filing_date\", \"date_published\", \"patent_issue_date\", \"abandon_date\"):\n",
    "            v = pred.get(k)\n",
    "            if v is None or (isinstance(v, str) and v.strip() == \"\"):\n",
    "                continue\n",
    "            try:\n",
    "                datetime.strptime(str(v).strip(), \"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                iso_ok = False\n",
    "                break\n",
    "        if iso_ok:\n",
    "            fmt = 0.1\n",
    "\n",
    "    w1, w2, w3, w4 = weights\n",
    "    total = w1 * validity + w2 * field_mean + w3 * constraints + w4 * fmt\n",
    "    total = max(0.0, min(1.0, float(total)))\n",
    "\n",
    "    return total, {\n",
    "        \"validity\": float(validity),\n",
    "        \"field_mean\": float(field_mean),\n",
    "        \"constraints\": float(constraints),\n",
    "        \"format\": float(fmt),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476d1939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, {'validity': 0.0, 'field_mean': 0.0, 'constraints': 0.0, 'format': 0.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward(\n",
    "    '{\"publication_umber\": \"US1234567A\", \"application_number\": \"US12/345,678\", \"patent_number\": \"1234567\", \"date_published\": \"2020-01-01\", \"filing_date\": \"2018-06-15\", \"patent_issue_date\": \"2021-05-20\", \"abandon_date\": \"\", \"decision\": \"granted\", \"main_cpc_label\": \"G06F17/30\", \"main_ipcr_label\": \"G06F17/30\", \"title\": \"Innovative Widget\", \"abstract\": \"An innovative widget that improves efficiency.\", }',\n",
    "    {\n",
    "        \"publication_number\": \"US1234567A\",\n",
    "        \"application_number\": \"US12/345,678\",\n",
    "        \"patent_number\": \"1234567\",\n",
    "        \"date_published\": \"2020-01-01\",\n",
    "        \"filing_date\": \"2018-06-15\",\n",
    "        \"patent_issue_date\": \"2021-05-20\",\n",
    "        \"abandon_date\": \"\",\n",
    "        \"decision\": \"granted\",\n",
    "        \"main_cpc_label\": \"G06F17/30\",\n",
    "        \"main_ipcr_label\": \"G06F17/30\",\n",
    "        \"title\": \"Innovative Widget\",\n",
    "        \"abstract\": \"An innovative widget that improves efficiency.\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bfc91",
   "metadata": {},
   "source": [
    "## SFT warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ad9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 590\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def get_prompt(text: str) -> str:\n",
    "    fields_str = \", \".join(RELEVANT_FIELDS)\n",
    "    return (\n",
    "        \"Extract the following fields as JSON only (no extra text). \"\n",
    "        f\"Fields: {{{fields_str}}}\\n\\n\"\n",
    "        f\"DOCUMENT:\\n{text}\\n\\n\"\n",
    "        \"Return strictly a single JSON object with those keys.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def df_to_sft_dataset(manifest_path: str, limit: int | None = None) -> Dataset:\n",
    "    df = pd.read_parquet(manifest_path)\n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(\"Manifest must contain a 'text' column. Re-run generateDatasets.py to pre-extract text.\")\n",
    "\n",
    "    prompts: List[str] = [get_prompt(t) for t in df[\"text\"].tolist()]\n",
    "\n",
    "    # Load gold JSON content as the target response\n",
    "    answers: List[str] = []\n",
    "    for p in df[\"gold_json_path\"].tolist():\n",
    "        with open(p, \"r\") as f:\n",
    "            answers.append(f.read())\n",
    "\n",
    "    # Single text field: prompt + answer delimited\n",
    "    samples = [\n",
    "        {\n",
    "            \"text\": f\"{prompt}\\n\\n<answer>\\n{answer}\\n</answer>\",\n",
    "        }\n",
    "        for prompt, answer in zip(prompts, answers)\n",
    "    ]\n",
    "    return Dataset.from_list(samples)\n",
    "\n",
    "train_ds = df_to_sft_dataset(\"Patent_Data/train_manifest.parquet\")\n",
    "val_ds = df_to_sft_dataset(\"Patent_Data/val_manifest.parquet\")\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ee918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 590/590 [00:00<00:00, 34804.08 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 590/590 [00:00<00:00, 1392.05 examples/s]\n",
      "Packing train dataset: 100%|██████████| 590/590 [00:00<00:00, 70186.61 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 107/107 [00:00<00:00, 43424.34 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 107/107 [00:00<00:00, 874.13 examples/s]\n",
      "Packing eval dataset: 100%|██████████| 107/107 [00:00<00:00, 54904.64 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 1:06:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=1.2435136159261069, metrics={'train_runtime': 4075.4271, 'train_samples_per_second': 0.116, 'train_steps_per_second': 0.007, 'total_flos': 986133719678976.0, 'train_loss': 1.2435136159261069, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False  # needed if you enable gradient checkpointing\n",
    "\n",
    "# LoRA config (common: q_proj, k_proj, v_proj, o_proj)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=\"qwen3_0p6B_lora\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.03,\n",
    "    packing=True,                # packs multiple samples into one sequence to save memory\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                  # pass the loaded model (not a string)\n",
    "    args=sft_args,\n",
    "    train_dataset=train_ds,       # your datasets\n",
    "    eval_dataset=val_ds,\n",
    "    peft_config=peft_config,      # attaches LoRA adapters\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c4ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qwen3_0p6B_lora_SFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1e0477",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model qwen3_0p6B_lora with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 989, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory qwen3_0p6B_lora.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 989, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory qwen3_0p6B_lora.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      4\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mIf you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m generator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqwen3_0p6B_lora\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m output = generator([{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question}], max_new_tokens=\u001b[32m128\u001b[39m, return_full_text=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(output[\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/pipelines/base.py:333\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback.items():\n\u001b[32m    332\u001b[39m             error += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    334\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    335\u001b[39m         )\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    338\u001b[39m     framework = infer_framework(model.\u001b[34m__class__\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not load model qwen3_0p6B_lora with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 989, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory qwen3_0p6B_lora.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mounselam/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 989, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory qwen3_0p6B_lora.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "question = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\n",
    "generator = pipeline(\"text-generation\", model=\"qwen3_0p6B_lora\")\n",
    "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\n",
    "print(output[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c8789",
   "metadata": {},
   "source": [
    "## RLVR GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e4a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aefef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
